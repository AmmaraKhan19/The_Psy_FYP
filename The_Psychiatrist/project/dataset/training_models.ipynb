{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Import Libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import re, string, joblib, nltk, json, collections, pandas as pd\r\n",
    "from sklearn import tree, metrics\r\n",
    "from sklearn.svm import SVC\r\n",
    "from sklearn.pipeline import Pipeline\r\n",
    "from textblob import Word \r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from sklearn.tree import DecisionTreeClassifier\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix, accuracy_score,classification_report\r\n",
    "from nltk.tokenize import RegexpTokenizer\r\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\r\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold\r\n",
    "from sklearn.ensemble import GradientBoostingClassifier\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "\r\n",
    "print(\"Libraries imported....\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Libraries imported....\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# ### Read csv file\r\n",
    "Suicide = pd.read_csv(\"training_dataset.csv\",encoding =\"ISO-8859-1\") "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Data Cleaning\n",
    "\n",
    "Data Cleaning - Removing Null, Missing Values, Renaming Columns."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "Suicide['Tweet']=Suicide['Tweet'].fillna(\"\")                  #remove all the null value"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# function to remove punctuation\r\n",
    "def remove_punct(text):\r\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\r\n",
    "    text = re.sub('[0-9]+', '', text)\r\n",
    "    return text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Data Preprocessing\n",
    "\n",
    "<ul><li>Lower-casing</li>\n",
    "    <li>NLTK</li> \n",
    "    <li>Removing Stop Words</li>\n",
    "    <li>Language Filtering</li>\n",
    "    <li>Lemmetization</li></ul>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "Suicide['lower_case']= Suicide['Tweet'].apply(lambda x: x.lower())   \r\n",
    "Suicide['tweet_punct'] = Suicide['lower_case'].apply(lambda x: remove_punct(x))   \r\n",
    "#Dataset['RT'] = Dataset['lower_case'].replace({\"rt\": ''}, regex=True)\r\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\r\n",
    "Suicide['Special_word'] = Suicide.apply(lambda row: tokenizer.tokenize(row['lower_case']), axis=1)    \r\n",
    "\r\n",
    "freq = pd.Series(' '.join(Suicide['Tweet']).split()).value_counts()[-10:]                       \r\n",
    "freq = list(freq.index)\r\n",
    "Suicide['Contents'] = Suicide['Tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq)) \r\n",
    "\r\n",
    "stop = stopwords.words('english')\r\n",
    "Suicide['stop_words'] = Suicide['Special_word'].apply(lambda x: [item for item in x if item not in stop])  \r\n",
    "\r\n",
    "Suicide['stop_words'] = Suicide['stop_words'].astype('str')\r\n",
    "Suicide['short_word'] = Suicide['stop_words'].str.findall('\\w{3,}')         \r\n",
    "Suicide['string'] = Suicide['stop_words'].replace({\"'\": '', ',': ''}, regex=True)\r\n",
    "Suicide['string'] = Suicide['string'].str.findall('\\w{3,}').str.join(' ') \r\n",
    " \r\n",
    "import nltk\r\n",
    "nltk.download('stopwords')\r\n",
    "words = set(nltk.corpus.words.words())\r\n",
    "Suicide['NonEnglish'] = Suicide['string'].apply(lambda x: \" \".join(x for x in x.split() if x in words))  \r\n",
    "\r\n",
    "Suicide['tweet'] = Suicide['NonEnglish'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ammar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Applying n-gram (1,3)\r\n",
    "\r\n",
    "Splitting the data into Train-Test ratio of 67-33. Applying n-gram (1,3) to Count Vectorizer and Fit-Transsform using Tf-IDF."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(Suicide[\"tweet\"],Suicide[\"Suicide\"], test_size = 0.33, random_state = 42)\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer    \r\n",
    "count_vect = CountVectorizer(ngram_range=(1, 3))               #set it to ngram (1,3)\r\n",
    "transformer = TfidfTransformer(norm='l2',sublinear_tf=True)\r\n",
    "\r\n",
    "x_train_counts = count_vect.fit_transform(x_train)\r\n",
    "x_train_tfidf = transformer.fit_transform(x_train_counts)\r\n",
    "\r\n",
    "x_test_counts = count_vect.transform(x_test)\r\n",
    "x_test_tfidf = transformer.transform(x_test_counts)\r\n",
    "\r\n",
    "print (x_train_tfidf.shape,x_test_tfidf.shape, y_train.shape, x_train.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(7187, 285056) (3540, 285056) (7187,) (7187,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Machine Learning Model\n",
    "\n",
    "Using various Machine learning classifiers to Train, Test and Predict and Validate them."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5.1 GradientBoostingClassifier \r\n",
    "\r\n",
    "Running the GradientBoostingClassifier with the following parameters and capturing the performance metrics."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier                       \r\n",
    "model_1 = GradientBoostingClassifier(n_estimators=1000,\r\n",
    "                                        max_features='auto', max_depth=4,\r\n",
    "                                        random_state=1, verbose=1)\r\n",
    "\r\n",
    "model_1.fit(x_train_tfidf, y_train)\r\n",
    "y_pred1 = model_1.predict(x_test_tfidf)\r\n",
    "from sklearn.metrics import accuracy_score,classification_report\r\n",
    "print(accuracy_score(y_test, y_pred1))\r\n",
    "print(classification_report(y_test, y_pred1))\r\n",
    "\r\n",
    "\r\n",
    "# Cross Validation\r\n",
    "\r\n",
    "scores_1 = cross_val_score(model_1, x_train_tfidf,y_train, cv=3)   #3 fold validation\r\n",
    "print(accuracy_score(y_test,y_pred1))\r\n",
    "print (\"Cross-validated scores:\", scores_1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.2764           16.31m\n",
      "         2           1.2064           17.60m\n",
      "         3           1.1492           17.29m\n",
      "         4           1.1001           16.48m\n",
      "         5           1.0586           15.98m\n",
      "         6           1.0233           16.33m\n",
      "         7           0.9923           16.78m\n",
      "         8           0.9656           16.81m\n",
      "         9           0.9377           16.44m\n",
      "        10           0.9165           17.15m\n",
      "        20           0.7589           16.58m\n",
      "        30           0.6780           14.97m\n",
      "        40           0.6360           13.78m\n",
      "        50           0.6056           13.39m\n",
      "        60           0.5815           13.19m\n",
      "        70           0.5618           12.93m\n",
      "        80           0.5445           12.66m\n",
      "        90           0.5298           12.40m\n",
      "       100           0.5164           12.10m\n",
      "       200           0.4266           10.17m\n",
      "       300           0.3721            9.05m\n",
      "       400           0.3349            7.44m\n",
      "       500           0.3055            6.21m\n",
      "       600           0.2810            4.77m\n",
      "       700           0.2602            3.69m\n",
      "       800           0.2406            2.45m\n",
      "       900           0.2247            1.21m\n",
      "      1000           0.2108            0.00s\n",
      "0.8824858757062147\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.90      2015\n",
      "           1       0.87      0.86      0.86      1525\n",
      "\n",
      "    accuracy                           0.88      3540\n",
      "   macro avg       0.88      0.88      0.88      3540\n",
      "weighted avg       0.88      0.88      0.88      3540\n",
      "\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.2788           40.51m\n",
      "         2           1.2111           43.13m\n",
      "         3           1.1548           41.28m\n",
      "         4           1.1075           38.51m\n",
      "         5           1.0671           36.97m\n",
      "         6           1.0327           35.78m\n",
      "         7           1.0029           34.88m\n",
      "         8           0.9728           34.13m\n",
      "         9           0.9484           33.57m\n",
      "        10           0.9252           33.09m\n",
      "        20           0.7709           31.41m\n",
      "        30           0.6862           31.29m\n",
      "        40           0.6437           30.05m\n",
      "        50           0.6133           30.74m\n",
      "        60           0.5867           33.43m\n",
      "        70           0.5672           34.71m\n",
      "        80           0.5493           35.58m\n",
      "        90           0.5317           35.10m\n",
      "       100           0.5177           33.93m\n",
      "       200           0.4197           27.12m\n",
      "       300           0.3619           23.61m\n",
      "       400           0.3210           20.69m\n",
      "       500           0.2886           17.32m\n",
      "       600           0.2618           13.50m\n",
      "       700           0.2396           10.11m\n",
      "       800           0.2195            6.76m\n",
      "       900           0.2035            3.41m\n",
      "      1000           0.1883            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.2734           28.84m\n",
      "         2           1.2006           30.48m\n",
      "         3           1.1410           31.05m\n",
      "         4           1.0906           32.73m\n",
      "         5           1.0476           33.42m\n",
      "         6           1.0101           33.79m\n",
      "         7           0.9775           33.64m\n",
      "         8           0.9493           33.49m\n",
      "         9           0.9242           33.00m\n",
      "        10           0.9021           32.48m\n",
      "        20           0.7377           32.02m\n",
      "        30           0.6556           31.23m\n",
      "        40           0.6139           30.94m\n",
      "        50           0.5838           30.33m\n",
      "        60           0.5584           29.86m\n",
      "        70           0.5384           29.41m\n",
      "        80           0.5198           29.07m\n",
      "        90           0.5046           28.67m\n",
      "       100           0.4901           28.25m\n",
      "       200           0.3930           24.32m\n",
      "       300           0.3382           21.98m\n",
      "       400           0.2979           20.18m\n",
      "       500           0.2658           16.25m\n",
      "       600           0.2402           12.72m\n",
      "       700           0.2178            9.37m\n",
      "       800           0.1988            6.17m\n",
      "       900           0.1820            3.05m\n",
      "      1000           0.1669            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.2748           28.67m\n",
      "         2           1.2039           28.59m\n",
      "         3           1.1456           28.45m\n",
      "         4           1.0960           28.40m\n",
      "         5           1.0547           28.30m\n",
      "         6           1.0188           28.32m\n",
      "         7           0.9868           28.54m\n",
      "         8           0.9595           28.47m\n",
      "         9           0.9341           28.46m\n",
      "        10           0.9124           28.63m\n",
      "        20           0.7558           28.36m\n",
      "        30           0.6745           28.36m\n",
      "        40           0.6271           27.81m\n",
      "        50           0.5947           27.33m\n",
      "        60           0.5704           26.91m\n",
      "        70           0.5484           26.65m\n",
      "        80           0.5290           26.24m\n",
      "        90           0.5137           25.86m\n",
      "       100           0.4988           25.52m\n",
      "       200           0.3977           22.41m\n",
      "       300           0.3384           19.45m\n",
      "       400           0.2988           16.58m\n",
      "       500           0.2670           13.75m\n",
      "       600           0.2422           10.98m\n",
      "       700           0.2201          236.77m\n",
      "       800           0.2011          139.43m\n",
      "       900           0.1840           62.42m\n",
      "      1000           0.1697            0.00s\n",
      "0.8824858757062147\n",
      "Cross-validated scores: [0.89649416 0.88105175 0.8822547 ]\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5.2 AdaBoost with RF\r\n",
    "\r\n",
    "AdaBoost with RandomForestClassifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\r\n",
    "dt = RandomForestClassifier(n_estimators=40, max_depth=9, random_state=0)\r\n",
    "model_2 = AdaBoostClassifier(base_estimator=dt, learning_rate=0.2, n_estimators=100)\r\n",
    "model_2.fit(x_train_tfidf, y_train)                                                   \r\n",
    "y_pred2 = model_2.predict(x_test_tfidf)\r\n",
    "print(accuracy_score(y_test, y_pred2))\r\n",
    "print(classification_report(y_test, y_pred2))\r\n",
    "\r\n",
    "# Cross-Validation\r\n",
    "\r\n",
    "scores_2 = cross_val_score(model_2, x_train_tfidf,y_train, cv=3)   #3 fold validation\r\n",
    "print(accuracy_score(y_test,y_pred2))\r\n",
    "print (\"Cross-validated scores:\", scores_2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8624293785310735\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.97      0.89      2015\n",
      "           1       0.95      0.71      0.82      1525\n",
      "\n",
      "    accuracy                           0.86      3540\n",
      "   macro avg       0.89      0.84      0.85      3540\n",
      "weighted avg       0.88      0.86      0.86      3540\n",
      "\n",
      "0.8624293785310735\n",
      "Cross-validated scores: [0.87353923 0.85350584 0.86096033]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Applying n-gram (1,2)\r\n",
    "\r\n",
    "Splitting the data into Train-Test ratio of 67-33. Applying n-gram(1,2) to Count Vectorizer and Fit-Transsform using Tf-IDF."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Suicide[\"tweet\"],Suicide[\"Suicide\"], test_size = 0.33, random_state = 42)\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\r\n",
    "count_vect = CountVectorizer(ngram_range=(1, 2))\r\n",
    "transformer = TfidfTransformer(norm='l2',sublinear_tf=True)\r\n",
    "\r\n",
    "X_train_counts = count_vect.fit_transform(X_train)\r\n",
    "X_train_tfidf = transformer.fit_transform(X_train_counts)\r\n",
    "\r\n",
    "X_test_counts = count_vect.transform(X_test)\r\n",
    "X_test_tfidf = transformer.transform(X_test_counts)\r\n",
    "\r\n",
    "print (X_train_tfidf.shape,X_test_tfidf.shape, y_train.shape, x_train.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Machine Learning Algorithm"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7.1 Random Forest and Adaboost\r\n",
    "\r\n",
    "RandomForestClassifier with AdaBoost."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "dt_stump = RandomForestClassifier(n_estimators=40, max_depth=9, random_state=0)\r\n",
    "model_a = AdaBoostClassifier(base_estimator=dt_stump, learning_rate=0.1, n_estimators=100)\r\n",
    "model_a.fit(X_train_tfidf, y_train)                                                   \r\n",
    "y_preda = model_a.predict(X_test_tfidf)\r\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_preda))\r\n",
    "print(\"Report:\", classification_report(y_test, y_preda))\r\n",
    "\r\n",
    "### Cross Validation\r\n",
    "\r\n",
    "scores_a = cross_val_score(model_a, X_train_tfidf,y_train, cv=3)   #3 fold validation\r\n",
    "print(\"Validation Accuracy: \", accuracy_score(y_test,y_preda))\r\n",
    "print (\"Cross-validated scores:\", scores_a, \"\\n\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy:  0.8559322033898306\n",
      "Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.98      0.89      2015\n",
      "           1       0.96      0.69      0.81      1525\n",
      "\n",
      "    accuracy                           0.86      3540\n",
      "   macro avg       0.89      0.84      0.85      3540\n",
      "weighted avg       0.87      0.86      0.85      3540\n",
      "\n",
      "Validation Accuracy:  0.8559322033898306\n",
      "Cross-validated scores: [0.86936561 0.8484975  0.85219207] \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7.2 Gradient Boosting\r\n",
    "\r\n",
    "Running the GradientBoostingClassifier with the following parameters and capturing the performance metrics."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier                       \r\n",
    "model_b = GradientBoostingClassifier(n_estimators=5000,\r\n",
    "                                        max_features='auto', max_depth=5,\r\n",
    "                                        random_state=1, verbose=1)\r\n",
    "\r\n",
    "model_b.fit(X_train_tfidf, y_train)\r\n",
    "y_predb = model_a.predict(X_test_tfidf)\r\n",
    "from sklearn.metrics import accuracy_score,classification_report\r\n",
    "print(accuracy_score(y_test, y_predb))\r\n",
    "print(classification_report(y_test, y_predb))\r\n",
    "\r\n",
    "#3 fold validation\r\n",
    "scores_b = cross_val_score(model_b, X_train_tfidf,y_train, cv=3)\r\n",
    "print(accuracy_score(y_test,y_predb))\r\n",
    "print (\"Cross-validated scores:\", scores_b)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.2702           33.54m\n",
      "         2           1.1957           33.04m\n",
      "         3           1.1343           34.09m\n",
      "         4           1.0821           33.82m\n",
      "         5           1.0376           33.46m\n",
      "         6           0.9997           33.54m\n",
      "         7           0.9666           33.31m\n",
      "         8           0.9376           33.32m\n",
      "         9           0.9074           33.08m\n",
      "        10           0.8834           33.14m\n",
      "        20           0.7196           32.82m\n",
      "        30           0.6349           32.48m\n",
      "        40           0.5852           31.80m\n",
      "        50           0.5559           30.60m\n",
      "        60           0.5332           29.84m\n",
      "        70           0.5139           29.68m\n",
      "        80           0.4944           29.44m\n",
      "        90           0.4805           28.96m\n",
      "       100           0.4674           28.58m\n",
      "       200           0.3753           25.90m\n",
      "       300           0.3248           24.27m\n",
      "       400           0.2893           23.08m\n",
      "       500           0.2606           22.76m\n",
      "       600           0.2365           22.11m\n",
      "       700           0.2167           21.36m\n",
      "       800           0.1985           20.83m\n",
      "       900           0.1835           20.42m\n",
      "      1000           0.1708           19.86m\n",
      "      2000           0.0922           14.64m\n",
      "      3000           0.0579            9.64m\n",
      "      4000           0.0406            4.99m\n",
      "      5000           0.0317            0.00s\n",
      "0.8559322033898306\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.98      0.89      2015\n",
      "           1       0.96      0.69      0.81      1525\n",
      "\n",
      "    accuracy                           0.86      3540\n",
      "   macro avg       0.89      0.84      0.85      3540\n",
      "weighted avg       0.87      0.86      0.85      3540\n",
      "\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.2733           61.46m\n",
      "         2           1.2020           61.64m\n",
      "         3           1.1423           62.32m\n",
      "         4           1.0917           65.41m\n",
      "         5           1.0488           67.27m\n",
      "         6           1.0116           68.32m\n",
      "         7           0.9787           69.38m\n",
      "         8           0.9444           69.13m\n",
      "         9           0.9157           68.21m\n",
      "        10           0.8919           67.55m\n",
      "        20           0.7295           66.60m\n",
      "        30           0.6431           66.10m\n",
      "        40           0.5951           65.47m\n",
      "        50           0.5637           64.56m\n",
      "        60           0.5386           64.92m\n",
      "        70           0.5171           64.43m\n",
      "        80           0.4970           64.08m\n",
      "        90           0.4805           64.45m\n",
      "       100           0.4658           64.57m\n",
      "       200           0.3709           60.66m\n",
      "       300           0.3153           62.01m\n",
      "       400           0.2742           62.45m\n",
      "       500           0.2409           59.72m\n",
      "       600           0.2156           57.44m\n",
      "       700           0.1935           55.42m\n",
      "       800           0.1750           53.50m\n",
      "       900           0.1600           51.67m\n",
      "      1000           0.1460           50.00m\n",
      "      2000           0.0691           36.50m\n",
      "      3000           0.0424           25.64m\n",
      "      4000           0.0311           12.82m\n",
      "      5000           0.0266            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.2668          128.30m\n",
      "         2           1.1897          130.71m\n",
      "         3           1.1249          161.87m\n",
      "         4           1.0711          155.94m\n",
      "         5           1.0242          151.81m\n",
      "         6           0.9852          146.59m\n",
      "         7           0.9512          145.32m\n",
      "         8           0.9193          150.09m\n",
      "         9           0.8932          170.07m\n",
      "        10           0.8641          168.65m\n",
      "        20           0.6944          171.79m\n",
      "        30           0.6071          176.29m\n",
      "        40           0.5622          177.73m\n",
      "        50           0.5326          172.49m\n",
      "        60           0.5081          176.71m\n",
      "        70           0.4871          169.77m\n",
      "        80           0.4701          163.90m\n",
      "        90           0.4536          159.95m\n",
      "       100           0.4386          155.60m\n",
      "       200           0.3421          140.29m\n",
      "       300           0.2894          131.22m\n",
      "       400           0.2507          121.36m\n",
      "       500           0.2192          116.88m\n",
      "       600           0.1943          115.03m\n",
      "       700           0.1731          113.83m\n",
      "       800           0.1552          111.24m\n",
      "       900           0.1398          108.15m\n",
      "      1000           0.1259          106.00m\n",
      "      2000           0.0553           81.20m\n",
      "      3000           0.0310           53.86m\n",
      "      4000           0.0214           27.01m\n",
      "      5000           0.0174            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.2685          196.88m\n",
      "         2           1.1928          222.15m\n",
      "         3           1.1307          228.56m\n",
      "         4           1.0786          226.46m\n",
      "         5           1.0326          228.96m\n",
      "         6           0.9938          227.65m\n",
      "         7           0.9601          227.63m\n",
      "         8           0.9287          224.72m\n",
      "         9           0.9013          220.00m\n",
      "        10           0.8767          222.09m\n",
      "        20           0.7119          212.74m\n",
      "        30           0.6233          199.43m\n",
      "        40           0.5737          192.21m\n",
      "        50           0.5395          192.81m\n",
      "        60           0.5148          196.29m\n",
      "        70           0.4924          196.86m\n",
      "        80           0.4728          191.92m\n",
      "        90           0.4560          188.45m\n",
      "       100           0.4405          183.31m\n",
      "       200           0.3409          164.41m\n",
      "       300           0.2872          146.99m\n",
      "       400           0.2481          137.97m\n",
      "       500           0.2172          131.58m\n",
      "       600           0.1924          127.01m\n",
      "       700           0.1726          123.28m\n",
      "       800           0.1552          122.08m\n",
      "       900           0.1413          117.36m\n",
      "      1000           0.1286          115.15m\n",
      "      2000           0.0609           84.80m\n",
      "      3000           0.0379           54.53m\n",
      "      4000           0.0289           27.02m\n",
      "      5000           0.0253            0.00s\n",
      "0.8559322033898306\n",
      "Cross-validated scores: [0.89816361 0.8827212  0.88601253]\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7.3 AdaBoost with DT\r\n",
    "\r\n",
    "AdaBoost with DecisionTreeClassifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from sklearn.tree import DecisionTreeClassifier                               \r\n",
    "from sklearn.ensemble import AdaBoostClassifier\r\n",
    "dt_stump = tree.DecisionTreeClassifier()\r\n",
    "model_i= AdaBoostClassifier(base_estimator=dt_stump, learning_rate=0.1, n_estimators=300)\r\n",
    "model_i.fit(X_train_tfidf, y_train)                                                   \r\n",
    "y_predi = model_i.predict(X_test_tfidf)\r\n",
    "print(accuracy_score(y_test, y_predi))\r\n",
    "print(classification_report(y_test, y_predi))\r\n",
    "\r\n",
    "scores_i = cross_val_score(model_i, X_train_tfidf,y_train, cv=3)   #3 fold validation\r\n",
    "print(accuracy_score(y_test,y_predi))\r\n",
    "print (\"Cross-validated scores:\", scores_i)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8062146892655367\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.86      0.83      2015\n",
      "           1       0.80      0.73      0.77      1525\n",
      "\n",
      "    accuracy                           0.81      3540\n",
      "   macro avg       0.81      0.80      0.80      3540\n",
      "weighted avg       0.81      0.81      0.80      3540\n",
      "\n",
      "0.8062146892655367\n",
      "Cross-validated scores: [0.86310518 0.83639399 0.8434238 ]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8. SVM Model\r\n",
    "\r\n",
    "#### Using SVM rbf kernel and pipeline to impliment steps in sequence."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "# split dataset into train-test set\r\n",
    "x_train, x_test,y_train, y_test = train_test_split(Suicide['tweet'].values.astype('str'), Suicide['Suicide'].values.astype('str'), test_size = 0.40,\r\n",
    "                                                   random_state = 400)\r\n",
    "# Steps implementation in series\r\n",
    "# Use pipeline to carry out steps in sequence with a single object\r\n",
    "# SVM's rbf kernel gives highest accuracy.\r\n",
    "\r\n",
    "clf_model = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', SVC(kernel = 'rbf'))])\r\n",
    "\r\n",
    "# Training\r\n",
    "\r\n",
    "clf_model.fit(x_train, y_train)\r\n",
    "\r\n",
    "# predict class form test data \r\n",
    "predict = clf_model.predict(x_test)\r\n",
    "\r\n",
    "# print accuracy and classification report\r\n",
    "accuracy = (accuracy_score(y_test, predict))\r\n",
    "print(\"Accuracy: \", accuracy)\r\n",
    "print(\"Report: \", classification_report(y_test, predict), \"\\n\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy:  0.8981589373106502\n",
      "Report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.92      2473\n",
      "           1       0.93      0.82      0.87      1818\n",
      "\n",
      "    accuracy                           0.90      4291\n",
      "   macro avg       0.90      0.89      0.89      4291\n",
      "weighted avg       0.90      0.90      0.90      4291\n",
      " \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 10. Save best model\r\n",
    "\r\n",
    "using joblib to save model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "save_model = 'svm_model.sav'\r\n",
    "joblib.dump(clf_model, save_model) # to save model\r\n",
    "print(\"Model saved to disk...\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model saved to disk...\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f218911d4b2f655d84802c485109f14f8ddc2e122e4c71a9cece3bef49479136"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}